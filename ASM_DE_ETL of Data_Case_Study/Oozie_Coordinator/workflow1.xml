<workflow-app xmlns="uri:oozie:workflow:0.2" name="cdw_sapp_update">
    <start to="sqoop-node"/>    
    <action name="sqoop-node">    <!--node name--> 
        <sqoop xmlns="uri:oozie:sqoop-action:0.2">
            <job-tracker>${jobTracker}</job-tracker>  <!--job tracking node is http://sandbox.hortonworks.com:8050 which is define in job properties--> 
            <name-node>${nameNode}</name-node> <!--name node is http://sandbox.hortonworks.com:8020 which is define in job properties--> 
              <prepare>
               <delete path="${nameNode}/user/maria_dev/Credit_Card_System/CDW_SAPP_CUSTOMER1"/>   <!--if oldmovies directory is present in hadoop then this line will delet oldmovies directory because when oldmovies.sql file will create this create directry again for new dataset   --> 
            </prepare>
            <configuration>
                <property>
                    <name>mapred.job.queue.name</name>
                    <value>${queueName}</value>
                </property>
            </configuration>
            <command>job  --meta-connect jdbc:hsqldb:hsql://localhost:16000/sqoop --exec incrementcust</command>
            <archive>/user/maria_dev/lib/java-json.jar#java-json.jar</archive>
          </sqoop>
        <ok to="sqoop-node2"/>
         <error to = "kill_job" />
    </action>

     <action name="sqoop-node2">    <!--node name--> 
        <sqoop xmlns="uri:oozie:sqoop-action:0.2">
            <job-tracker>${jobTracker}</job-tracker>  <!--job tracking node is http://sandbox.hortonworks.com:8050 which is define in job properties--> 
            <name-node>${nameNode}</name-node> <!--name node is http://sandbox.hortonworks.com:8020 which is define in job properties--> 
              <prepare>
               <delete path="${nameNode}/user/maria_dev/Credit_Card_System/CDW_SAPP_BRANCH1"/>   <!--if oldmovies directory is present in hadoop then this line will delet oldmovies directory because when oldmovies.sql file will create this create directry again for new dataset   --> 
            </prepare>
              <configuration>
                <property>
                    <name>mapred.job.queue.name</name>
                    <value>${queueName}</value>
                </property>
            </configuration>
            <command>job  --meta-connect jdbc:hsqldb:hsql://localhost:16000/sqoop --exec incrementbranch</command>
           <archive>/user/maria_dev/lib/java-json.jar#java-json.jar</archive>
          </sqoop>
        <ok to="sqoop-node3"/>
         <error to =  "kill_job" />
    </action>

    <action name="sqoop-node3">    <!--node name--> 
        <sqoop xmlns="uri:oozie:sqoop-action:0.2">
            <job-tracker>${jobTracker}</job-tracker>  <!--job tracking node is http://sandbox.hortonworks.com:8050 which is define in job properties--> 
            <name-node>${nameNode}</name-node> <!--name node is http://sandbox.hortonworks.com:8020 which is define in job properties--> 
             <prepare>
               <delete path="${nameNode}/user/maria_dev/Credit_Card_System/CDW_SAPP_CREDITCARD1"/>   <!--if oldmovies directory is present in hadoop then this line will delet oldmovies directory because when oldmovies.sql file will create this create directry again for new dataset   --> 
            </prepare>
              <configuration>
                <property>
                    <name>mapred.job.queue.name</name>
                    <value>${queueName}</value>
                </property>
            </configuration>
            <command>job  --meta-connect jdbc:hsqldb:hsql://localhost:16000/sqoop --exec incrementCC</command>
             <archive>/user/maria_dev/lib/java-json.jar#java-json.jar</archive>
          </sqoop>
        <ok to="sqoop-node4"/>
         <error to =  "kill_job" />
    </action>

    <action name="sqoop-node4">    <!--node name--> 
        <sqoop xmlns="uri:oozie:sqoop-action:0.2">
            <job-tracker>${jobTracker}</job-tracker>  <!--job tracking node is http://sandbox.hortonworks.com:8050 which is define in job properties--> 
            <name-node>${nameNode}</name-node> <!--name node is http://sandbox.hortonworks.com:8020 which is define in job properties--> 
              <prepare>
               <delete path="${nameNode}/user/maria_dev/Credit_Card_System/CDW_SAPP_TIMEID1"/>   <!--if oldmovies directory is present in hadoop then this line will delet oldmovies directory because when oldmovies.sql file will create this create directry again for new dataset   --> 
            </prepare>
              <configuration>
                <property>
                    <name>mapred.job.queue.name</name>
                    <value>${queueName}</value>
                </property>
            </configuration>
            <command>job  --meta-connect jdbc:hsqldb:hsql://localhost:16000/sqoop --exec incrementTime</command>
             <archive>/user/maria_dev/lib/java-json.jar#java-json.jar</archive>
            
          </sqoop>
        <ok to="Do_Initial_load_and_Transformation_in_HIVE"/>
         <error to =  "kill_job" />
    </action>
 
    
   <action name = "Do_Initial_load_and_Transformation_in_HIVE">
      <hive xmlns = "uri:oozie:hive-action:0.4">
        <job-tracker>${jobTracker}</job-tracker>
         <name-node>${nameNode}</name-node>

         <script>${nameNode}/user/maria_dev/Case_Study_WorkflowOptmz/LoadandTransformation.hive</script>
      </hive>
	      <ok to = "Insert_data" />
      <error to = "kill_job" />
   </action>
   

   <action name = "Insert_data">
      <hive xmlns = "uri:oozie:hive-action:0.4">
         <job-tracker>${jobTracker}</job-tracker>
         <name-node>${nameNode}</name-node>

         <script>${nameNode}/user/maria_dev/Case_Study_WorkflowOptmz/Insert.hive</script>
        
      </hive>
      <ok to = "end" />
      <error to = "kill_job" />
	</action>

 
   <kill name = "kill_job">
      <message>Job failed</message>
   </kill>
	
   <end name = "end" />
</workflow-app>
